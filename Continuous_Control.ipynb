{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "You are welcome to use this coding environment to train your agent for the project.  Follow the instructions below to get started!\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "Run the next code cell to install a few packages.  This line will take a few minutes to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip -q install ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check GPU Status\n",
    "\n",
    "This command displays information about the NVIDIA GPU available in the environment:\n",
    "- GPU device name and ID\n",
    "- Driver and CUDA version \n",
    "- Memory usage and capacity\n",
    "- GPU utilization and temperature\n",
    "\n",
    "This helps verify that the GPU is properly configured before training the deep learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Nov 16 08:41:05 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   44C    P8             12W /   70W |       1MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environments corresponding to both versions of the environment are already saved in the Workspace and can be accessed at the file paths provided below.  \n",
    "\n",
    "Please select one of the two options below for loading the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found path: /data/Reacher_One_Linux_NoVis/Reacher_One_Linux_NoVis.x86_64\n",
      "Mono path[0] = '/data/Reacher_One_Linux_NoVis/Reacher_One_Linux_NoVis_Data/Managed'\n",
      "Mono config path = '/data/Reacher_One_Linux_NoVis/Reacher_One_Linux_NoVis_Data/MonoBleedingEdge/etc'\n",
      "Preloaded 'libgrpc_csharp_ext.x64.so'\n",
      "Unable to preload the following plugins:\n",
      "\tlibgrpc_csharp_ext.x86.so\n",
      "Logging to /home/student/.config/unity3d/Unity Technologies/Unity Environment/Player.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_size -> 5.0\n",
      "\t\tgoal_speed -> 1.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "# select this option to load version 1 (with a single agent) of the environment\n",
    "env = UnityEnvironment(file_name='/data/Reacher_One_Linux_NoVis/Reacher_One_Linux_NoVis.x86_64')\n",
    "\n",
    "# select this option to load version 2 (with 20 agents) of the environment\n",
    "# env = UnityEnvironment(file_name='/data/Reacher_Linux_NoVis/Reacher.x86_64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Size of each action: 4\n",
      "There are 1 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726671e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Note that **in this coding environment, you will not be able to watch the agents while they are training**, and you should set `train_mode=True` to restart the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env_info = env.reset(train_mode=True)[brain_name]      # reset the environment    \n",
    "# states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "# scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "# while True:\n",
    "#     actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "#     actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "#     env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "#     next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "#     rewards = env_info.rewards                         # get reward (for each agent)\n",
    "#     dones = env_info.local_done                        # see if episode finished\n",
    "#     scores += env_info.rewards                         # update the score (for each agent)\n",
    "#     states = next_states                               # roll over states to next time step\n",
    "#     if np.any(dones):                                  # exit loop if episode finished\n",
    "#         break\n",
    "# print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  A few **important notes**:\n",
    "- When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```\n",
    "- To structure your work, you're welcome to work directly in this Jupyter notebook, or you might like to start over with a new file!  You can see the list of files in the workspace by clicking on **_Jupyter_** in the top left corner of the notebook.\n",
    "- In this coding environment, you will not be able to watch the agents while they are training.  However, **_after training the agents_**, you can download the saved model weights to watch the agents on your own machine! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Required Libraries\n",
    "\n",
    "This cell imports all the necessary Python libraries and modules needed for the deep reinforcement learning implementation:\n",
    "\n",
    "- **System and Utility Libraries**:\n",
    "  - `sys`: For system-specific parameters and functions\n",
    "  - `time`: For time-related functions\n",
    "  - `copy`: For creating deep copies of objects\n",
    "  - `random`: For random number generation\n",
    "  - `collections`: For specialized container datatypes (deque, namedtuple)\n",
    "\n",
    "- **Deep Learning Framework**:\n",
    "  - `torch`: PyTorch base library\n",
    "  - `torch.nn`: Neural network modules\n",
    "  - `torch.nn.functional`: Neural network functions\n",
    "  - `torch.optim`: Optimization algorithms\n",
    "\n",
    "- **Numerical and Visualization**:\n",
    "  - `numpy`: For numerical computations\n",
    "  - `matplotlib.pyplot`: For plotting training results\n",
    "  - `%matplotlib inline`: Jupyter magic command to display plots inline\n",
    "\n",
    "These libraries provide the foundation for implementing the DDPG (Deep Deterministic Policy Gradient) algorithm, handling data structures, and visualizing results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay Buffer Implementation\n",
    "\n",
    "This class implements Experience Replay, a critical component of modern deep reinforcement learning algorithms. The Replay Buffer stores and manages past experiences for more efficient and stable learning.\n",
    "\n",
    "Key features:\n",
    "- Uses a double-ended queue (deque) with fixed maximum size to store experiences\n",
    "- Each experience is a tuple containing (state, action, reward, next_state, done)\n",
    "- Supports random sampling of experiences for batch learning\n",
    "- Handles conversion of experiences to PyTorch tensors and device placement\n",
    "\n",
    "Main methods:\n",
    "- `__init__`: Initializes the buffer with specified size and batch parameters\n",
    "- `add`: Stores new experiences in the buffer\n",
    "- `sample`: Randomly samples a batch of experiences and converts them to tensors\n",
    "- `__len__`: Returns current number of experiences in buffer\n",
    "\n",
    "The buffer helps break correlations in sequential data and enables the agent to learn from past experiences multiple times, improving sample efficiency and training stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from typing import Tuple, List, Any\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"A fixed-size buffer to store experience tuples for deep reinforcement learning.\n",
    "    \n",
    "    This implementation provides efficient storage and sampling of experience tuples\n",
    "    (state, action, reward, next_state, done) for training deep RL agents. It uses\n",
    "    a double-ended queue with a maximum size to automatically handle buffer overflow.\n",
    "    \n",
    "    Attributes:\n",
    "        action_size (int): Dimension of each action\n",
    "        memory (deque): Double-ended queue storing experiences\n",
    "        batch_size (int): Size of each training batch\n",
    "        experience (namedtuple): Template for storing experience tuples\n",
    "        device (torch.device): Device for tensor operations\n",
    "        \n",
    "    Example:\n",
    "        >>> buffer = ReplayBuffer(action_size=4, buffer_size=100000, batch_size=64, seed=42)\n",
    "        >>> state = env.reset()\n",
    "        >>> action = agent.act(state)\n",
    "        >>> next_state, reward, done, _ = env.step(action)\n",
    "        >>> buffer.add(state, action, reward, next_state, done)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, action_size: int, buffer_size: int, batch_size: int, seed: int):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        \n",
    "        Args:\n",
    "            action_size (int): Dimension of each action\n",
    "            buffer_size (int): Maximum size of buffer\n",
    "            batch_size (int): Size of each training batch\n",
    "            seed (int): Random seed for reproducibility\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", \n",
    "                                   field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def add(self, state: np.ndarray, action: np.ndarray, reward: float, \n",
    "            next_state: np.ndarray, done: bool) -> None:\n",
    "        \"\"\"Add a new experience to memory.\n",
    "        \n",
    "        Args:\n",
    "            state: Current state observation\n",
    "            action: Action taken\n",
    "            reward: Reward received\n",
    "            next_state: Next state observation\n",
    "            done: Whether the episode ended\n",
    "        \"\"\"\n",
    "        experience = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(experience)\n",
    "\n",
    "    def sample(self) -> Tuple[torch.Tensor, ...]:\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\n",
    "        \n",
    "        Returns:\n",
    "            tuple: Batch of (states, actions, rewards, next_states, dones)\n",
    "                  Each element is a torch tensor moved to the appropriate device\n",
    "        \"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "        \n",
    "        # Filter out None experiences and convert to tensors\n",
    "        valid_experiences = [e for e in experiences if e is not None]\n",
    "        \n",
    "        states = torch.from_numpy(\n",
    "            np.vstack([e.state for e in valid_experiences])\n",
    "        ).float().to(self.device)\n",
    "        \n",
    "        actions = torch.from_numpy(\n",
    "            np.vstack([e.action for e in valid_experiences])\n",
    "        ).float().to(self.device)\n",
    "        \n",
    "        rewards = torch.from_numpy(\n",
    "            np.vstack([e.reward for e in valid_experiences])\n",
    "        ).float().to(self.device)\n",
    "        \n",
    "        next_states = torch.from_numpy(\n",
    "            np.vstack([e.next_state for e in valid_experiences])\n",
    "        ).float().to(self.device)\n",
    "        \n",
    "        dones = torch.from_numpy(\n",
    "            np.vstack([e.done for e in valid_experiences]).astype(np.uint8)\n",
    "        ).float().to(self.device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor Network Architecture\n",
    "\n",
    "This class implements the Actor network component of the DDPG algorithm, which learns to predict the optimal action for any given state.\n",
    "\n",
    "Network structure:\n",
    "- **Input Layer**: Takes state vectors as input\n",
    "- **Hidden Layers**: Two fully connected layers with 128 units each\n",
    "- **Output Layer**: Produces action vectors with tanh activation\n",
    "- **Batch Normalization**: Applied after first hidden layer for training stability\n",
    "\n",
    "Key features:\n",
    "- ReLU activation functions in hidden layers\n",
    "- Tanh activation in output layer to bound actions between [-1, 1]\n",
    "- Batch normalization to prevent internal covariate shift\n",
    "- Configurable hidden layer sizes (default: 128 units)\n",
    "\n",
    "The Actor network is essential in DDPG as it represents the policy π(s) that maps states to deterministic actions. The tanh activation ensures the output actions are properly scaled for the continuous control task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Tuple\n",
    "\n",
    "class ActorNetwork(nn.Module):\n",
    "    \"\"\"Actor (Policy) Network for DDPG algorithm.\n",
    "    \n",
    "    This network maps states to deterministic actions using a feed-forward neural network\n",
    "    with batch normalization. The network architecture consists of three fully connected\n",
    "    layers with ReLU activations and a final tanh activation to bound the actions.\n",
    "    \n",
    "    Attributes:\n",
    "        fc1 (nn.Linear): First fully connected layer\n",
    "        fc2 (nn.Linear): Second fully connected layer\n",
    "        fc3 (nn.Linear): Output layer\n",
    "        bn1 (nn.BatchNorm1d): Batch normalization after first layer\n",
    "        \n",
    "    Example:\n",
    "        >>> actor = ActorNetwork(state_size=33, action_size=4, seed=0)\n",
    "        >>> state = torch.randn(1, 33)\n",
    "        >>> action = actor(state)  # Returns action values between -1 and 1\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 state_size: int, \n",
    "                 action_size: int, \n",
    "                 seed: int, \n",
    "                 hidden_size1: int = 128, \n",
    "                 hidden_size2: int = 128) -> None:\n",
    "        \"\"\"Initialize the Actor Network.\n",
    "        \n",
    "        Args:\n",
    "            state_size: Dimension of each state\n",
    "            action_size: Dimension of each action\n",
    "            seed: Random seed for reproducibility\n",
    "            hidden_size1: Number of nodes in first hidden layer\n",
    "            hidden_size2: Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        \n",
    "        # Set random seed for reproducibility\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        \n",
    "        # Network layers\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size1)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size1)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.fc3 = nn.Linear(hidden_size2, action_size)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "        \n",
    "    def _initialize_weights(self) -> None:\n",
    "        \"\"\"Initialize network weights using Xavier initialization.\"\"\"\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        nn.init.xavier_uniform_(self.fc3.weight)\n",
    "\n",
    "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Build an actor (policy) network that maps states -> actions.\n",
    "        \n",
    "        Args:\n",
    "            state: Current state tensor of shape (batch_size, state_size)\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Action values bounded between -1 and 1\n",
    "        \"\"\"\n",
    "        # First hidden layer with batch normalization\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = self.bn1(x)\n",
    "        \n",
    "        # Second hidden layer\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        # Output layer with tanh activation to bound actions\n",
    "        return F.tanh(self.fc3(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Critic Network Architecture\n",
    "\n",
    "This class implements the Critic network component of the DDPG algorithm, which learns to estimate the Q-value (expected future rewards) for state-action pairs.\n",
    "\n",
    "Network structure:\n",
    "- **Input Processing**:\n",
    "  - State path: First processes the state through an initial hidden layer\n",
    "  - Action path: Actions are concatenated with processed state features\n",
    "- **Architecture**:\n",
    "  - First hidden layer (128 units) processes state only\n",
    "  - Batch normalization for training stability\n",
    "  - Second hidden layer (128 units) processes combined state-action features\n",
    "  - Output layer produces single Q-value estimate\n",
    "\n",
    "Key features:\n",
    "- ReLU activations in hidden layers\n",
    "- Batch normalization for stable training\n",
    "- State-action fusion through concatenation\n",
    "- Linear output for Q-value estimation\n",
    "\n",
    "The Critic network is crucial for DDPG as it provides value estimates that guide the Actor's policy updates. Its architecture is specifically designed to effectively combine state and action information for accurate Q-value prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Tuple\n",
    "\n",
    "class CriticNetwork(nn.Module):\n",
    "    \"\"\"Critic (Value) Network for DDPG algorithm.\n",
    "    \n",
    "    This network estimates the Q-value for state-action pairs using a feed-forward neural network\n",
    "    with batch normalization. The network architecture consists of three fully connected layers\n",
    "    with ReLU activations.\n",
    "    \n",
    "    Attributes:\n",
    "        fc1 (nn.Linear): First fully connected layer for state input\n",
    "        fc2 (nn.Linear): Second fully connected layer for combined state-action input\n",
    "        fc3 (nn.Linear): Output layer producing a single Q-value\n",
    "        bn1 (nn.BatchNorm1d): Batch normalization after first layer\n",
    "        \n",
    "    Example:\n",
    "        >>> critic = CriticNetwork(state_size=33, action_size=4, seed=0)\n",
    "        >>> state = torch.randn(1, 33)\n",
    "        >>> action = torch.randn(1, 4)\n",
    "        >>> q_value = critic(state, action)  # Returns estimated Q-value\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 state_size: int, \n",
    "                 action_size: int, \n",
    "                 seed: int, \n",
    "                 hidden_size1: int = 128, \n",
    "                 hidden_size2: int = 128) -> None:\n",
    "        \"\"\"Initialize the Critic Network.\n",
    "        \n",
    "        Args:\n",
    "            state_size: Dimension of each state\n",
    "            action_size: Dimension of each action\n",
    "            seed: Random seed for reproducibility\n",
    "            hidden_size1: Number of nodes in first hidden layer\n",
    "            hidden_size2: Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(CriticNetwork, self).__init__()\n",
    "        \n",
    "        # Set random seed for reproducibility\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        \n",
    "        # Network layers\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size1)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size1)\n",
    "        self.fc2 = nn.Linear(hidden_size1 + action_size, hidden_size2)\n",
    "        self.fc3 = nn.Linear(hidden_size2, 1)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "        \n",
    "    def _initialize_weights(self) -> None:\n",
    "        \"\"\"Initialize network weights using Xavier initialization.\"\"\"\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        nn.init.xavier_uniform_(self.fc3.weight)\n",
    "\n",
    "    def forward(self, state: torch.Tensor, action: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Build a critic (value) network that maps (state, action) pairs -> Q-values.\n",
    "        \n",
    "        Args:\n",
    "            state: Current state tensor of shape (batch_size, state_size)\n",
    "            action: Action tensor of shape (batch_size, action_size)\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Estimated Q-value for the given state-action pair\n",
    "        \"\"\"\n",
    "        # First hidden layer with batch normalization\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = self.bn1(x)\n",
    "        \n",
    "        # Concatenate state and action, then pass through second hidden layer\n",
    "        x = torch.cat([x, action], dim=1)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        # Output layer producing Q-value\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ornstein-Uhlenbeck Noise Implementation\n",
    "\n",
    "This class implements the Ornstein-Uhlenbeck process for generating temporally correlated noise, which is crucial for exploration in continuous action spaces.\n",
    "\n",
    "Key components:\n",
    "- **Process Parameters**:\n",
    "  - `mu`: Mean of the noise process (default: 0.0)\n",
    "  - `theta`: Rate of mean reversion (default: 0.1)\n",
    "  - `sigma`: Scale of noise (default: 0.2)\n",
    "  - `sigma_decay`: Decay rate for noise scale (default: 0.99)\n",
    "  - `sigma_min`: Minimum noise scale (default: 0.1)\n",
    "\n",
    "Main methods:\n",
    "- `reset`: Resets noise state and decays sigma\n",
    "- `sample`: Generates next noise sample using the OU process equation\n",
    "\n",
    "Features:\n",
    "- Temporally correlated noise generation\n",
    "- Adaptive noise scaling through decay mechanism\n",
    "- Configurable parameters for exploration control\n",
    "\n",
    "The OU noise process is particularly suitable for continuous control tasks as it provides smooth, temporally correlated exploration, which is more appropriate than random noise for physical systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reference: https://github.com/xkiwilabs/Multi-Agent-DDPG-using-PTtorch-and-ML-Agents/blob/master/OUNoise.py\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "from typing import Any\n",
    "\n",
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process for generating noise in continuous action spaces.\n",
    "    \n",
    "    This process generates temporally correlated noise, which is useful for exploration\n",
    "    in continuous action spaces. It is particularly suited for physical control problems\n",
    "    where smooth exploration is desired.\n",
    "    \n",
    "    Attributes:\n",
    "        mu (np.ndarray): Long-term mean of the noise process\n",
    "        theta (float): Rate of mean reversion\n",
    "        sigma (float): Scale of the noise\n",
    "        sigma_min (float): Minimum value for sigma\n",
    "        sigma_decay (float): Decay rate for sigma\n",
    "        state (np.ndarray): Current state of the noise process\n",
    "        \n",
    "    Example:\n",
    "        >>> noise = OUNoise(size=4, seed=0)\n",
    "        >>> action = np.array([0.5, 0.1, -0.3, 0.0])\n",
    "        >>> noisy_action = action + noise.sample()\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 size: int, \n",
    "                 seed: int, \n",
    "                 mu: float = 0.0, \n",
    "                 theta: float = 0.1, \n",
    "                 sigma: float = 0.2, \n",
    "                 sigma_min: float = 0.1, \n",
    "                 sigma_decay: float = 0.99) -> None:\n",
    "        \"\"\"Initialize parameters and noise process.\n",
    "        \n",
    "        Args:\n",
    "            size: Dimension of the noise\n",
    "            seed: Random seed for reproducibility\n",
    "            mu: Long-term mean of the noise process\n",
    "            theta: Rate of mean reversion\n",
    "            sigma: Initial scale of the noise\n",
    "            sigma_min: Minimum value for sigma\n",
    "            sigma_decay: Decay rate for sigma\n",
    "        \"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.sigma_min = sigma_min\n",
    "        self.sigma_decay = sigma_decay\n",
    "        self.seed = random.seed(seed)\n",
    "        self.size = size\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        \"\"\"Reset the internal state to the mean (mu) and decay sigma.\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "        self.sigma = max(self.sigma_min, self.sigma * self.sigma_decay)\n",
    "\n",
    "    def sample(self) -> np.ndarray:\n",
    "        \"\"\"Update internal state and return it as a noise sample.\n",
    "        \n",
    "        Returns:\n",
    "            np.ndarray: Noise sample to be added to actions\n",
    "        \"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.standard_normal(self.size)\n",
    "        self.state = x + dx\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Configuration\n",
    "\n",
    "This cell defines the key hyperparameters used in the DDPG algorithm:\n",
    "\n",
    "- **Learning Parameters**:\n",
    "  - `LR = 3E-4`: Learning rate for both Actor and Critic networks\n",
    "  - `GAMMA = 0.99`: Discount factor for future rewards\n",
    "  - `TAU = 1e-3`: Soft update coefficient for target networks\n",
    "\n",
    "- **Memory and Batch Settings**:\n",
    "  - `BATCH_SIZE = 64`: Number of experiences to sample for each learning update\n",
    "  - `BUFFER_SIZE = 1000000`: Maximum size of the replay buffer\n",
    "\n",
    "These hyperparameters are crucial for the algorithm's performance:\n",
    "- The learning rate balances learning speed and stability\n",
    "- Gamma determines how much future rewards are valued\n",
    "- TAU controls how quickly target networks track local networks\n",
    "- Buffer size and batch size affect learning efficiency and stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 3E-4\n",
    "GAMMA = 0.99\n",
    "BATCH_SIZE = 64\n",
    "TAU = 1e-3\n",
    "BUFFER_SIZE = 1000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Device Configuration\n",
    "\n",
    "This line configures the computation device for PyTorch operations:\n",
    "\n",
    "- Checks if CUDA-enabled GPU is available using `torch.cuda.is_available()`\n",
    "- If GPU is available, sets device to \"cuda:0\" (first CUDA device)\n",
    "- If no GPU is available, falls back to \"cpu\"\n",
    "\n",
    "This configuration ensures optimal performance by:\n",
    "- Utilizing GPU acceleration when available for faster neural network computations\n",
    "- Providing automatic fallback to CPU processing when GPU is not available\n",
    "- Maintaining code compatibility across different hardware setups\n",
    "\n",
    "The selected device will be used for all tensor operations and neural network computations in the DDPG implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPG Agent Implementation\n",
    "\n",
    "This class implements the Deep Deterministic Policy Gradient (DDPG) agent, combining actor-critic architecture with experience replay and noise-based exploration.\n",
    "\n",
    "# Deep Deterministic Policy Gradient (DDPG) Algorithm\n",
    "\n",
    "DDPG is an off-policy algorithm that combines insights from DQN and deterministic policy gradients to handle continuous action spaces.\n",
    "\n",
    "## Core Components\n",
    "\n",
    "### 1. Actor-Critic Architecture\n",
    "- **Actor** μ(s|θ<sup>μ</sup>): Implements deterministic policy\n",
    "  - Maps states to specific actions\n",
    "  - Gradient: ∇<sub>θ<sup>μ</sup></sub>J ≈ E<sub>s~ρ<sup>β</sup></sub>[∇<sub>a</sub>Q(s,a|θ<sup>Q</sup>)|<sub>a=μ(s)</sub> ∇<sub>θ<sup>μ</sup></sub>μ(s|θ<sup>μ</sup>)]\n",
    "\n",
    "- **Critic** Q(s,a|θ<sup>Q</sup>): Estimates action-value function\n",
    "  - Updates using Bellman equation\n",
    "  - Target: y<sub>i</sub> = r<sub>i</sub> + γQ'(s<sub>i+1</sub>, μ'(s<sub>i+1</sub>)|θ<sup>Q'</sup>)\n",
    "\n",
    "### 2. Network Updates\n",
    "- **Critic Update**: Minimizes loss L = 1/N Σ(y<sub>i</sub> - Q(s<sub>i</sub>,a<sub>i</sub>|θ<sup>Q</sup>))<sup>2</sup>\n",
    "- **Actor Update**: Maximizes expected Q-value through policy gradient\n",
    "- **Target Networks**: Updated softly with parameter τ\n",
    "  - θ' ← τθ + (1-τ)θ'\n",
    "\n",
    "### 3. Exploration Strategy\n",
    "- Adds Ornstein-Uhlenbeck noise to deterministic actions\n",
    "- dx<sub>t</sub> = θ(μ-x<sub>t</sub>)dt + σdW<sub>t</sub>\n",
    "  - θ: Mean reversion strength\n",
    "  - μ: Long-term mean\n",
    "  - σ: Noise scale\n",
    "\n",
    "### 4. Experience Replay\n",
    "- Stores transitions (s<sub>t</sub>, a<sub>t</sub>, r<sub>t</sub>, s<sub>t+1</sub>)\n",
    "- Breaks correlations in experience sequence\n",
    "- Enables mini-batch learning\n",
    "\n",
    "## Algorithm Flow\n",
    "1. Observe state s<sub>t</sub>\n",
    "2. Execute action a<sub>t</sub> = μ(s<sub>t</sub>) + N<sub>t</sub>\n",
    "3. Receive reward r<sub>t</sub> and new state s<sub>t+1</sub>\n",
    "4. Store transition in replay buffer\n",
    "5. Sample random mini-batch\n",
    "6. Update critic by minimizing loss\n",
    "7. Update actor using policy gradient\n",
    "8. Soft update target networks\n",
    "\n",
    "This architecture enables stable learning in continuous action spaces while maintaining sample efficiency through experience replay and exploration through noise injection.\n",
    "\n",
    "Key Components:\n",
    "- **Networks**:\n",
    "  - Actor (Local & Target): Determines optimal actions for states\n",
    "  - Critic (Local & Target): Evaluates state-action pairs\n",
    "  - Both use Adam optimizer with configured learning rate\n",
    "\n",
    "- **Core Methods**:\n",
    "  - `act`: Selects actions using current policy and exploration noise\n",
    "  - `step`: Stores experiences and triggers learning\n",
    "  - `learn`: Updates both networks using sampled experiences\n",
    "  - `update_target_network`: Performs soft updates of target networks\n",
    "\n",
    "Features:\n",
    "- Separate target networks for stability\n",
    "- Experience replay buffer for sample efficiency\n",
    "- Ornstein-Uhlenbeck noise for exploration\n",
    "- Soft target updates with TAU parameter\n",
    "\n",
    "The agent integrates all components of DDPG:\n",
    "- Policy and value function approximation\n",
    "- Experience storage and replay\n",
    "- Exploration mechanism\n",
    "- Network synchronization\n",
    "- Gradient-based learning updates\n",
    "\n",
    "This implementation follows the original DDPG algorithm design while incorporating best practices for deep reinforcement learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from typing import Any\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"DDPG Agent for interacting with and learning from the environment.\n",
    "    \n",
    "    This agent uses an actor-critic architecture with experience replay and\n",
    "    Ornstein-Uhlenbeck noise for exploration in continuous action spaces.\n",
    "    \n",
    "    Attributes:\n",
    "        actor_local (ActorNetwork): Local actor network\n",
    "        actor_target (ActorNetwork): Target actor network\n",
    "        actor_optimizer (optim.Optimizer): Optimizer for actor network\n",
    "        critic_local (CriticNetwork): Local critic network\n",
    "        critic_target (CriticNetwork): Target critic network\n",
    "        critic_optimizer (optim.Optimizer): Optimizer for critic network\n",
    "        memory (ReplayBuffer): Replay buffer for experience storage\n",
    "        exploration_noise (OUNoise): Noise process for exploration\n",
    "        \n",
    "    Example:\n",
    "        >>> agent = Agent(state_size=33, action_size=4, seed=0)\n",
    "        >>> state = env.reset()\n",
    "        >>> action = agent.act(state)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_size: int, action_size: int, seed: int) -> None:\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Args:\n",
    "            state_size: Dimension of each state\n",
    "            action_size: Dimension of each action\n",
    "            seed: Random seed for reproducibility\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "        # Actor networks\n",
    "        self.actor_local = ActorNetwork(state_size, action_size, seed).to(device)\n",
    "        self.actor_target = ActorNetwork(state_size, action_size, seed).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=LR)\n",
    "\n",
    "        # Critic networks\n",
    "        self.critic_local = CriticNetwork(state_size, action_size, seed).to(device)\n",
    "        self.critic_target = CriticNetwork(state_size, action_size, seed).to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=LR)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
    "\n",
    "        # OUNoise\n",
    "        self.exploration_noise = OUNoise(action_size, seed)\n",
    "\n",
    "    def act(self, state: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Returns actions for given state as per current policy.\n",
    "        \n",
    "        Args:\n",
    "            state: Current state observation\n",
    "            \n",
    "        Returns:\n",
    "            np.ndarray: Action values clipped between -1 and 1\n",
    "        \"\"\"\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        \n",
    "        self.actor_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action = self.actor_local(state).cpu().data.numpy()\n",
    "        self.actor_local.train()\n",
    "\n",
    "        # Add exploration noise\n",
    "        action += self.exploration_noise.sample()\n",
    "\n",
    "        return np.clip(action, -1, 1)\n",
    "\n",
    "    def step(self, state: np.ndarray, action: np.ndarray, reward: float, \n",
    "             next_state: np.ndarray, done: bool) -> None:\n",
    "        \"\"\"Save experience in replay memory and use random sample from buffer to learn.\"\"\"\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        if len(self.memory) > BATCH_SIZE:\n",
    "            self.learn()\n",
    "\n",
    "    def learn(self) -> None:\n",
    "        \"\"\"Update policy and value parameters using batch of experience tuples.\"\"\"\n",
    "        states, actions, rewards, next_states, dones = self.memory.sample()\n",
    "        next_actions = self.actor_target(next_states)\n",
    "        y = rewards + GAMMA * self.critic_target(next_states, next_actions) * (1 - dones)\n",
    "        \n",
    "        # Critic loss\n",
    "        critic_loss = F.mse_loss(y, self.critic_local(states, actions))\n",
    "        \n",
    "        # Critic backprop \n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # Actor loss\n",
    "        cur_actions = self.actor_local(states)\n",
    "        actor_loss = -self.critic_local(states, cur_actions).mean()\n",
    "\n",
    "        # Actor backprop\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # Soft updates\n",
    "        self.update_target_network()\n",
    "\n",
    "    def update_target_network(self) -> None:\n",
    "        \"\"\"Soft update model parameters.\"\"\"\n",
    "        for target_param, local_param in zip(self.actor_target.parameters(), self.actor_local.parameters()):\n",
    "            target_param.data.copy_(TAU * local_param.data + (1.0 - TAU) * target_param.data)\n",
    "\n",
    "        for target_param, local_param in zip(self.critic_target.parameters(), self.critic_local.parameters()):\n",
    "            target_param.data.copy_(TAU * local_param.data + (1.0 - TAU) * target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Initialization\n",
    "\n",
    "This line creates an instance of the DDPG agent with the following parameters:\n",
    "\n",
    "- **state_size**: Dimension of the state space (33 values per agent)\n",
    "  - Includes position, rotation, velocity, and angular velocities\n",
    "  - Represents the complete observable state of the robotic arm\n",
    "\n",
    "- **action_size**: Dimension of the action space (4 values per agent)\n",
    "  - Controls torque applicable to two arm joints\n",
    "  - Each action is a continuous value between -1 and 1\n",
    "\n",
    "- **seed**: Random seed (42) for reproducibility\n",
    "  - Ensures consistent initialization of:\n",
    "    - Neural network weights\n",
    "    - Replay buffer sampling\n",
    "    - Exploration noise generation\n",
    "\n",
    "This initialization sets up all necessary components:\n",
    "- Actor and Critic networks (both local and target)\n",
    "- Experience replay buffer\n",
    "- Ornstein-Uhlenbeck noise process\n",
    "- Optimizers for both networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size=state_size, action_size=action_size, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Function Implementation\n",
    "\n",
    "This function implements the main training loop for the DDPG agent in the continuous control environment. It handles episode execution, score tracking, and model checkpointing.\n",
    "\n",
    "Key Parameters:\n",
    "- `n_episodes=1000`: Maximum number of training episodes\n",
    "- `max_t=1000`: Maximum timesteps per episode\n",
    "\n",
    "Main Components:\n",
    "- **Score Tracking**:\n",
    "  - `scores`: List of all episode scores\n",
    "  - `scores_window`: Rolling window of last 100 scores\n",
    "  - Calculates running mean for performance monitoring\n",
    "\n",
    "- **Episode Loop**:\n",
    "  1. Resets exploration noise for new episode\n",
    "  2. Initializes environment and gets initial state\n",
    "  3. Executes timestep loop:\n",
    "     - Agent selects action\n",
    "     - Environment steps forward\n",
    "     - Agent learns from experience\n",
    "     - Accumulates rewards\n",
    "\n",
    "- **Checkpointing**:\n",
    "  - Monitors rolling average score\n",
    "  - Saves model weights when performance threshold (30) is reached\n",
    "  - Stores separate checkpoints for actor and critic networks\n",
    "\n",
    "The function provides continuous feedback on training progress and automatically terminates when the desired performance level is achieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 172, Average score: 30.01"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from collections import deque\n",
    "from typing import List\n",
    "\n",
    "def train(n_episodes: int = 1000, max_t: int = 1000) -> List[float]:\n",
    "    \"\"\"Train the DDPG agent in the environment.\n",
    "    \n",
    "    Args:\n",
    "        n_episodes: Maximum number of training episodes\n",
    "        max_t: Maximum number of timesteps per episode\n",
    "        \n",
    "    Returns:\n",
    "        List[float]: Scores obtained in each episode\n",
    "    \"\"\"\n",
    "    scores = []  # List to store scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # Last 100 scores for calculating average\n",
    "\n",
    "    for i_episode in range(1, n_episodes + 1):\n",
    "        agent.exploration_noise.reset()  # Reset noise for each episode\n",
    "        \n",
    "        env_info = env.reset(train_mode=True)[brain_name]  # Reset environment\n",
    "        state = env_info.vector_observations[0]  # Get initial state\n",
    "        score = 0  # Initialize score for the episode\n",
    "        \n",
    "        for _ in range(max_t):\n",
    "            action = agent.act(state)  # Get action from agent\n",
    "            env_info = env.step(action)[brain_name]  # Take action in environment\n",
    "            next_state = env_info.vector_observations[0]  # Get next state\n",
    "            reward = env_info.rewards[0]  # Get reward\n",
    "            done = env_info.local_done[0]  # Check if episode is done\n",
    "            \n",
    "            agent.step(state, action, reward, next_state, done)  # Agent learns from experience\n",
    "            state = next_state  # Move to next state\n",
    "            score += reward  # Accumulate reward\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        scores.append(score)  # Save most recent score\n",
    "        scores_window.append(score)  # Save score to window\n",
    "        mean_score = np.mean(scores_window)  # Calculate average score\n",
    "        \n",
    "        print(f\"\\rEpisode: {i_episode}, Average Score: {mean_score:.2f}\", end=\"\")\n",
    "        \n",
    "        # Save model if average score is above threshold\n",
    "        if mean_score >= 30.0:\n",
    "            torch.save(agent.actor_local.state_dict(), 'actor_checkpoint.pth')\n",
    "            torch.save(agent.critic_local.state_dict(), 'critic_checkpoint.pth')\n",
    "            print(f\"\\nEnvironment solved in {i_episode} episodes!\")\n",
    "            break\n",
    "    \n",
    "    return scores\n",
    "\n",
    "scores = train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Performance Visualization\n",
    "\n",
    "This code block creates a plot to visualize the agent's learning progress over time:\n",
    "\n",
    "- **Plot Configuration**:\n",
    "  - X-axis: Episode number\n",
    "  - Y-axis: Score achieved in each episode\n",
    "  - Single line plot showing score progression\n",
    "\n",
    "- **Components**:\n",
    "  - `fig = plt.figure()`: Creates new figure instance\n",
    "  - `ax = fig.add_subplot(111)`: Adds single subplot\n",
    "  - `plt.plot()`: Plots scores against episode numbers\n",
    "  - Axis labels for clarity: 'Score' and 'Episode #'\n",
    "\n",
    "- **Training Results**:\n",
    "  - Achieved target performance (30.01) in 172 episodes\n",
    "  - Indicates successful learning:\n",
    "    - Efficient convergence rate\n",
    "    - Stable learning progression\n",
    "    - Effective exploration-exploitation balance\n",
    "    - Well-tuned hyperparameters\n",
    "\n",
    "The visualization confirms that the DDPG agent successfully learned to control the robotic arm, reaching the target performance threshold relatively quickly and demonstrating the effectiveness of the implemented architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook Continuous_Control.ipynb to html\n",
      "[NbConvertApp] WARNING | Alternative text is missing on 1 image(s).\n",
      "[NbConvertApp] Writing 438909 bytes to Continuous_Control.html\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to html Continuous_Control.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Unity ML)",
   "language": "python",
   "name": "unity-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
